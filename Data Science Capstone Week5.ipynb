{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Battles of Neighbourhoods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Exploring New York geographical coordiantes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "import csv # implements classes to read and write tabular data in CSV form\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!wget -q -O 'newyork_data.json' https://ibm.box.com/shared/static/fbpwbovar7lf8p5sgddm06cgipa2rxpe.json\n",
    "#print('Data downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newyork_data.json') as json_data:\n",
    "    newyork_data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in neighborhoods_data:\n",
    "    borough = neighborhood_name = data['properties']['borough'] \n",
    "    neighborhood_name = data['properties']['name']\n",
    "        \n",
    "    neighborhood_latlon = data['geometry']['coordinates']\n",
    "    neighborhood_lat = neighborhood_latlon[1]\n",
    "    neighborhood_lon = neighborhood_latlon[0]\n",
    "    \n",
    "    neighborhoods = neighborhoods.append({'Borough': borough,\n",
    "                                          'Neighborhood': neighborhood_name,\n",
    "                                          'Latitude': neighborhood_lat,\n",
    "                                          'Longitude': neighborhood_lon}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Web Scrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# conda install -c anaconda beautiful-soup --yes\n",
    "from bs4 import BeautifulSoup # package for parsing HTML and XML documents\n",
    "\n",
    "import csv # implements classes to read and write tabular data in CSV form\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "website_url = requests.get('https://en.wikipedia.org/wiki/Demographics_of_New_York_City').text\n",
    "soup = BeautifulSoup(website_url,'lxml')\n",
    "table = soup.find('table',{'class':'wikitable sortable'})\n",
    "#print(soup.prettify())\n",
    "\n",
    "headers = [header.text for header in table.find_all('th')]\n",
    "\n",
    "table_rows = table.find_all('tr')        \n",
    "rows = []\n",
    "for row in table_rows:\n",
    "   td = row.find_all('td')\n",
    "   row = [row.text for row in td]\n",
    "   rows.append(row)\n",
    "\n",
    "with open('BON2_POPULATION1.csv', 'w') as f:\n",
    "   writer = csv.writer(f)\n",
    "   writer.writerow(headers)\n",
    "   writer.writerows(row for row in rows if row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data=pd.read_csv('BON2_POPULATION1.csv')\n",
    "Pop_data.drop(Pop_data.columns[[7,8,9,10,11]], axis=1,inplace=True)\n",
    "print('Data downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing whitespaces and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data.columns = Pop_data.columns.str.replace(' ', '')\n",
    "Pop_data.columns = Pop_data.columns.str.replace('\\'','')\n",
    "Pop_data.rename(columns={'Borough':'persons_sq_mi','County':'persons_sq_km'}, inplace=True)\n",
    "Pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data.rename(columns = {'NewYorkCitysfiveboroughsvte\\n' : 'Borough',\n",
    "                   'Jurisdiction\\n':'County',\n",
    "                   'Population\\n':'Estimate_2017', \n",
    "                   'Landarea\\n':'square_miles',\n",
    "                    'Density\\n':'square_km'}, inplace=True)\n",
    "Pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data['Borough']=Pop_data['Borough'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['County']=Pop_data['County'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['Estimate_2017']=Pop_data['Estimate_2017'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['square_miles']=Pop_data['square_miles'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['square_km']=Pop_data['square_km'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['persons_sq_mi']=Pop_data['persons_sq_mi'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data['persons_sq_km']=Pop_data['persons_sq_km'].replace(to_replace='\\n', value='', regex=True)\n",
    "Pop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_data.loc[5:,['persons_sq_mi','persons_sq_km']] = Pop_data.loc[2:,['persons_sq_mi','persons_sq_km']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['square_km','persons_sq_mi']] = Pop_data.loc[2:,['square_km','persons_sq_mi']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['square_miles','square_km']] = Pop_data.loc[2:,['square_miles','square_km']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['Estimate_2017','square_miles']] = Pop_data.loc[2:,['Estimate_2017','square_miles']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['County','Estimate_2017']] = Pop_data.loc[2:,['County','Estimate_2017']].shift(1,axis=1)\n",
    "Pop_data.loc[5:,['Borough','County']] = Pop_data.loc[2:,['Borough','County']].shift(1,axis=1)\n",
    "Pop_data\n",
    "\n",
    "#Removing NAN\n",
    "Pop_data = Pop_data.fillna('')\n",
    "Pop_data\n",
    "\n",
    "#Saving the data\n",
    "Pop_data.to_csv('BON2_POPULATION.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_url = requests.get('https://en.wikipedia.org/wiki/New_York_City').text\n",
    "soup = BeautifulSoup(website_url,'lxml')\n",
    "table = soup.find('table',{'class':'wikitable sortable collapsible'})\n",
    "#print(soup.prettify())\n",
    "\n",
    "headers = [header.text for header in table.find_all('th')]\n",
    "\n",
    "table_rows = table.find_all('tr')        \n",
    "rows = []\n",
    "for row in table_rows:\n",
    "   td = row.find_all('td')\n",
    "   row = [row.text for row in td]\n",
    "   rows.append(row)\n",
    "\n",
    "with open('NYC_DEMO.csv', 'w') as f:\n",
    "   writer = csv.writer(f)\n",
    "   writer.writerow(headers)\n",
    "   writer.writerows(row for row in rows if row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demo_data=pd.read_csv('NYC_DEMO.csv')\n",
    "print('Data downloaded!')\n",
    "\n",
    "\n",
    "Demo_data.rename(columns = {'2010[237]' : '2010',\n",
    "                   '1990[239]':'1990',\n",
    "                   '1970[239]':'1970', \n",
    "                   '1940[239]\\n':'1940',\n",
    "                    }, inplace=True)\n",
    "\n",
    "Demo_data.to_csv('BON2_DEMOGRAPHICS.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Exploring the Cuisine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version:  3.0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "import pandas as pd # library for data analsysis\n",
    "from PIL import Image # converting images into arrays\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.style.use('ggplot') # optional: for ggplot-like style\n",
    "\n",
    "# check for latest version of Matplotlib\n",
    "print ('Matplotlib version: ', mpl.__version__) # >= 2.0.0\n",
    "\n",
    "# install wordcloud\n",
    "!conda install -c conda-forge wordcloud==1.4.1 --yes\n",
    "\n",
    "# import package and its set of stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "print ('Wordcloud is installed and imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file.seek(0)\n",
    "import pandas as pd\n",
    "NYC_CUISINE=pd.read_csv(my_file)\n",
    "NYC_CUISINE.drop(NYC_CUISINE.columns[[3,4,5,6,7]], axis=1,inplace=True) \n",
    "NYC_CUISINE.head()\n",
    "\n",
    "CUISINE_WC = NYC_CUISINE[['Cuisine']]\n",
    "CUISINE_WC\n",
    "\n",
    "plt.imshow(NYC_CUISINE_WC, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(30)\n",
    "fig.set_figheight(45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Exploring Farmers Market Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# notice: installing seaborn might takes a few minutes\n",
    "!conda install -c anaconda seaborn -y\n",
    "import seaborn as sns\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = project.get_file(\"DOHMH_Farmers_Markets_and_Food_Boxes.csv\")\n",
    "\n",
    "# Read the CSV data file from the object storage into a pandas DataFrame\n",
    "my_file.seek(0)\n",
    "FM_NYC=pd.read_csv(my_file)\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "sns.countplot(x='Service_Type',data=FM_NYC)\n",
    "ax.set_title(\"Service_Type\")\n",
    "for t in ax.patches:\n",
    "    if (np.isnan(float(t.get_height()))):\n",
    "        ax.annotate('', (t.get_x(), 0))\n",
    "    else:\n",
    "        ax.annotate(str(format(int(t.get_height()), ',d')), (t.get_x(), t.get_height()*1.01))\n",
    "    \n",
    "plt.show();\n",
    "\n",
    "FM_NYC_filtered = FM_NYC[FM_NYC['Service_Type'] == 'Farmers Markets'].copy()\n",
    "FM_NYC_filtered ['Borough'] = FM_NYC_filtered['Borough'].map(lambda x: x.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map of New York City using latitude and longitude values\n",
    "map_markets = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, FacilityName, borough in zip(FM_NYC_filtered['Latitude'], FM_NYC_filtered['Longitude'], FM_NYC_filtered['FacilityName'], FM_NYC_filtered['Borough']):\n",
    "            label = '{}, {}'.format(FacilityName, borough)\n",
    "            label = folium.Popup(label, parse_html=True)\n",
    "            folium.CircleMarker(\n",
    "                [lat, lng],\n",
    "                radius=5,\n",
    "                popup=label,\n",
    "                color='green',\n",
    "                fill=True,\n",
    "                fill_color='green',\n",
    "                fill_opacity=0.7,\n",
    "                parse_html = False).add_to(map_markets)  \n",
    "\n",
    "map_markets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 Segmenting and Clutering Neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC_Geo=pd.read_csv('BON1_NYC_GEO.csv')\n",
    "print('Data downloaded!')\n",
    "\n",
    "\n",
    "NYC_Geo.head()\n",
    "\n",
    "NYC_Geo['Borough'].value_counts().to_frame()\n",
    "\n",
    "BM_Geo = NYC_Geo.loc[(NYC_Geo['Borough'] == 'Brooklyn')|(NYC_Geo['Borough'] == 'Manhattan')]\n",
    "BM_Geo = BM_Geo.reset_index(drop=True)\n",
    "BM_Geo.head()\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "address = 'New York City, NY'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"Jupyter\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of New York City are {}, {}.'.format(latitude, longitude))\n",
    "\n",
    "print(\"--- %s seconds ---\" % round((time.time() - start_time), 2))\n",
    "\n",
    "map_BM = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, borough, neighborhood in zip(BM_Geo['Latitude'], BM_Geo['Longitude'], BM_Geo['Borough'], BM_Geo['Neighborhood']):\n",
    "    label = '{}, {}'.format(neighborhood, borough)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_BM)  \n",
    "    \n",
    "map_BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_BM = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, borough, neighborhood in zip(BM_Geo['Latitude'], BM_Geo['Longitude'], BM_Geo['Borough'], BM_Geo['Neighborhood']):\n",
    "    label = '{}, {}'.format(neighborhood, borough)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_BM)  \n",
    "    \n",
    "map_BM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Foursquare Credentials and Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'OLN1BAQQBHO234LKFIU1ZNGV4Z3O3P1GS5KIMTNPJHLX1MKL' # your Foursquare ID\n",
    "CLIENT_SECRET = 'VDM5CGGVSUOGKMY21ETO4J1UAJH5QJEALQCJAIWUF2DJXR2T' # your Foursquare Secret\n",
    "VERSION = '20181218' # Foursquare API version\n",
    "\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, LIMIT=200, radius=1000):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "     return(nearby_venues) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM_venues = getNearbyVenues(names=BM_Geo['Neighborhood'],\n",
    "                                  latitudes=BM_Geo['Latitude'],\n",
    "                                  longitudes=BM_Geo['Longitude'],\n",
    "                                  LIMIT=200)\n",
    "\n",
    "print('The \"BM_venues\" dataframe has {} venues and {} unique venue types.'.format(\n",
    "      len(BM_venues['Venue Category']),\n",
    "      len(BM_venues['Venue Category'].unique())))\n",
    "\n",
    "BM_venues.to_csv('BM_venues.csv', sep=',', encoding='UTF8')\n",
    "BM_venues.head()\n",
    "\n",
    "colnames = ['Neighborhood', 'Neighborhood Latitude', 'Neighborhood Longitude', 'Venue', 'Venue Latitude', 'Venue Longitude', 'Venue Category']\n",
    "BM_venues = pd.read_csv('BM_venues.csv', skiprows=1, names=colnames)\n",
    "BM_venues.columns = BM_venues.columns.str.replace(' ', '')\n",
    "BM_venues.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Venues_Map(Borough_name, Borough_neighborhoods):\n",
    "    \n",
    "    # Use geopy library to get the latitude and longitude values \n",
    "    geolocator = Nominatim(user_agent=\"Jupyter\")\n",
    "    Borough_location = geolocator.geocode(Borough_name) #'Brooklyn, NY'\n",
    "    Borough_latitude = Borough_location.latitude\n",
    "    Borough_longitude = Borough_location.longitude\n",
    "    print('The geographical coordinates of \"{}\" are {}, {}.'.format(Borough_name, Borough_latitude, Borough_longitude))\n",
    "    \n",
    "    # To verify the number of Boroughs and Neighborhoods in the extracted data\n",
    "    print('The \"{}\" dataframe has {} different venue types and {} neighborhoods.'.format(\n",
    "          Borough_name,\n",
    "          len(Borough_neighborhoods['VenueCategory'].unique()),\n",
    "          len(Borough_neighborhoods['Neighborhood'].unique())))\n",
    "    \n",
    "    # create map of city using latitude and longitude values\n",
    "    map_Borough = folium.Map(location=[Borough_latitude, Borough_longitude], zoom_start=10)\n",
    "\n",
    "    # add markers to map\n",
    "    for lat, lng, venue, category in zip(Borough_neighborhoods['VenueLatitude'], Borough_neighborhoods['VenueLongitude'], Borough_neighborhoods['Venue'], Borough_neighborhoods['VenueCategory']):\n",
    "        label = '{}, {}'.format(category, venue)\n",
    "        label = folium.Popup(label, parse_html=True)\n",
    "        folium.CircleMarker(\n",
    "            [lat, lng],\n",
    "            radius=0.1,\n",
    "            popup=label,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_color='#FF0000',\n",
    "            fill_opacity=0.3).add_to(map_Borough)  \n",
    "\n",
    "    return map_Borough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "BM_onehot = pd.get_dummies(BM_venues[['VenueCategory']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "#column lists before adding neighborhood\n",
    "column_names = ['Neighborhood'] + list(BM_onehot.columns)\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "BM_onehot['Neighborhood'] = BM_venues['Neighborhood'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "BM_onehot = BM_onehot[column_names]\n",
    "\n",
    "BM_onehot.head()\n",
    "\n",
    "restaurant_List = []\n",
    "search = 'Restaurant'\n",
    "for i in BM_onehot.columns :\n",
    "    if search in i:\n",
    "        restaurant_List.append(i)\n",
    "        \n",
    "        \n",
    "col_name = []\n",
    "col_name = ['Neighborhood'] + restaurant_List\n",
    "BM_restaurant = BM_onehot[col_name]\n",
    "BM_restaurant = BM_restaurant.iloc[:,1::]\n",
    "\n",
    "\n",
    "BM_restaurant_grouped['Total'] = BM_restaurant_grouped .sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Neighbourhood and Examine Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM_grouped_clustering = BM_restaurant_grouped.drop('Neighborhood', 1)\n",
    "\n",
    "for n_cluster in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=n_cluster).fit(BM_grouped_clustering)\n",
    "    label = kmeans.labels_\n",
    "    sil_coeff = silhouette_score(BM_grouped_clustering, label, metric='euclidean')\n",
    "    print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))\n",
    "    \n",
    "# set number of clusters\n",
    "kclusters = 2\n",
    "\n",
    "BM_grouped_clustering = BM_restaurant_grouped.drop('Neighborhood', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(BM_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_\n",
    "\n",
    "BM_results = pd.DataFrame(kmeans.cluster_centers_)\n",
    "BM_results.columns = BM_grouped_clustering.columns\n",
    "BM_results.index = ['cluster0','cluster1']\n",
    "BM_results['Total Sum'] = BM_results.sum(axis = 1)\n",
    "BM_results\n",
    "\n",
    "BM_results_merged = pd.DataFrame(BM_restaurant_grouped['Neighborhood'])\n",
    "\n",
    "BM_results_merged['Total'] = BM_restaurant_grouped['Total']\n",
    "BM_results_merged = BM_results_merged.assign(Cluster_Labels = kmeans.labels_)\n",
    "\n",
    "\n",
    "print(BM_results_merged.shape)\n",
    "BM_results_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i+x+(i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(BM_merged['Latitude'], BM_merged['Longitude'], BM_merged['Neighborhood'], BM_merged['Cluster_Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
